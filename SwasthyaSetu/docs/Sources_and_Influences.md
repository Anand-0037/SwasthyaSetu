# Sources and Influences

This document lists the key papers, articles, and code repositories that have influenced the design and development of the Medical Summarizer - Dual Perspective system.

## Papers

1.  **Attention Is All You Need** (Vaswani et al., 2017): The foundational paper that introduced the Transformer architecture, which is the basis for our summarization model.
    -   *URL*: https://arxiv.org/abs/1706.03762

2.  **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al., 2018): This paper introduced the BERT model, which has been highly influential in the field of NLP and provides a strong foundation for understanding transformer-based models.
    -   *URL*: https://arxiv.org/abs/1810.04805

3.  **Get To The Point: Summarization with Pointer-Generator Networks** (See et al., 2017): This paper introduces a pointer-generator network that can copy words from the source text, which helps with factual accuracy and is a key feature of our model.
    -   *URL*: https://arxiv.org/abs/1704.04368

4.  **PubMedQA: A Dataset for Biomedical Research Question Answering** (Jin et al., 2019): This paper introduces the PubMedQA dataset, which will be a primary data source for our project.
    -   *URL*: https://arxiv.org/abs/1909.06146

5.  **MEDIQA: A Challenge for Medical Question Answering and Summarization** (Savery et al., 2020): This paper describes the MEDIQA challenge and dataset, which is another key data source for our project.
    -   *URL*: https://aclanthology.org/2020.acl-main.493/

6.  **Investigating and Improving Faithfulness of Medical Summarization** (Sun et al., 2023): This paper introduces the FaMeSumm framework, which we will draw inspiration from for our faithfulness classifier.
    -   *URL*: https://arxiv.org/abs/2311.02271

## Code Repositories

1.  **Hugging Face Transformers**: A comprehensive library of pre-trained transformer models and tools for working with them. We will use this as a reference for our from-scratch implementation.
    -   *URL*: https://github.com/huggingface/transformers

2.  **The Annotated Transformer**: A detailed explanation and implementation of the Transformer model in PyTorch.
    -   *URL*: http://nlp.seas.harvard.edu/2018/04/03/attention.html

3.  **FastAPI**: The official repository for the FastAPI framework, which we will use for our backend.
    -   *URL*: https://github.com/tiangolo/fastapi

4.  **React**: The official repository for the React framework, which we will use for our frontend.
    -   *URL*: https://github.com/facebook/react


