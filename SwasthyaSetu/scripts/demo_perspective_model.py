import torch
import json
from model.perspective_aware_transformer import TinyPerspectiveAwareMedicalSummarizer
from utils.tokenizer import MedicalTokenizer

def create_demo_model():
    """
    Create and initialize a demo model with random weights for demonstration
    """
    try:
        tokenizer = MedicalTokenizer.load("./model/medical_tokenizer.json")
    except FileNotFoundError:
        print("Creating demo tokenizer...")
        # Create a minimal demo tokenizer
        tokenizer = MedicalTokenizer(vocab_size=1000)
        demo_texts = [
            "What is diabetes and how to manage it?",
            "Patient needs simple explanation about blood sugar control.",
            "Clinical management of type 2 diabetes mellitus includes medication and lifestyle modifications.",
            "Side effects of chemotherapy treatment for cancer patients.",
            "Understanding depression symptoms and treatment options."
        ]
        tokenizer.build_vocab(demo_texts)
        tokenizer.save("./model/demo_tokenizer.json")
    
    vocab_size = tokenizer.vocab_size_actual
    model = TinyPerspectiveAwareMedicalSummarizer(
        vocab_size=vocab_size,
        d_model=128,  # Smaller for demo
        num_heads=4,
        num_encoder_layers=2,
        num_decoder_layers=2,
        d_ff=256,
        dropout=0.1
    )
    
    return model, tokenizer

def demonstrate_perspective_awareness():
    """
    Demonstrate how the model generates different summaries for different perspectives
    """
    print("="*80)
    print("MEDICAL SUMMARIZER - DUAL PERSPECTIVE DEMONSTRATION")
    print("="*80)
    
    model, tokenizer = create_demo_model()
    model.eval()
    
    # Demo medical Q&A contexts
    demo_contexts = [
        {
            "question": "What is diabetes?",
            "context": "Diabetes is a chronic medical condition where blood glucose levels are too high. It occurs when the pancreas doesn't produce enough insulin or when the body cannot effectively use the insulin it produces. There are two main types: Type 1 diabetes is an autoimmune condition, while Type 2 diabetes is often related to lifestyle factors and genetics."
        },
        {
            "question": "What are the side effects of chemotherapy?",
            "context": "Chemotherapy can cause various side effects including nausea, vomiting, hair loss, fatigue, increased infection risk due to low white blood cell count, bleeding problems, mouth sores, and neuropathy. The severity and type of side effects depend on the specific drugs used, dosage, and individual patient factors."
        },
        {
            "question": "How to manage high blood pressure?",
            "context": "High blood pressure management involves lifestyle modifications such as reducing sodium intake, regular exercise, maintaining healthy weight, limiting alcohol consumption, and managing stress. Medications like ACE inhibitors, diuretics, or calcium channel blockers may be prescribed. Regular monitoring and follow-up with healthcare providers is essential."
        }
    ]
    
    with torch.no_grad():
        for i, item in enumerate(demo_contexts):
            print(f"\nExample {i+1}:")
            print(f"Question: {item['question']}")
            print(f"Medical Context: {item['context'][:100]}...")
            print("-" * 60)
            
            # Tokenize the context
            full_text = f"Question: {item['question']} Context: {item['context']}"
            src_tokens = tokenizer.encode(full_text, max_length=128, padding='max_length', return_tensors='pt')
            
            # Generate patient-friendly summary (perspective=1)
            print("üè• PATIENT PERSPECTIVE (Simplified, Accessible):")
            try:
                patient_summary_tokens = model.generate_summary(src_tokens, max_length=32, perspective=1, temperature=0.7)
                patient_summary = tokenizer.decode(patient_summary_tokens[0].tolist())
                print(f"   {patient_summary}")
            except Exception as e:
                print(f"   [Demo] Patient-friendly explanation about {item['question'].lower()}")
            
            # Generate clinician summary (perspective=2)
            print("üë®‚Äç‚öïÔ∏è CLINICIAN PERSPECTIVE (Technical, Detailed):")
            try:
                clinician_summary_tokens = model.generate_summary(src_tokens, max_length=32, perspective=2, temperature=0.7)
                clinician_summary = tokenizer.decode(clinician_summary_tokens[0].tolist())
                print(f"   {clinician_summary}")
            except Exception as e:
                print(f"   [Demo] Clinical assessment and management of {item['question'].lower()}")
            
            print("-" * 60)
    
    print("\n" + "="*80)
    print("KEY FEATURES OF THE PERSPECTIVE-AWARE MODEL:")
    print("="*80)
    print("‚úì Dual Perspective Generation: Patient-friendly vs Clinical summaries")
    print("‚úì Perspective Embeddings: Model learns different writing styles")
    print("‚úì Attention Mechanisms: Focus on relevant information per perspective")
    print("‚úì Copy Mechanism: Preserves important medical terms when needed")
    print("‚úì Provenance Tracking: Shows which source information was used")
    print("‚úì Faithfulness Scoring: Measures how well summary reflects source")
    print("‚úì ONNX Export: Ready for deployment in web applications")
    
    print("\n" + "="*80)
    print("MODEL ARCHITECTURE HIGHLIGHTS:")
    print("="*80)
    print("‚Ä¢ Perspective-Aware Embeddings: Separate embeddings for patient/clinician modes")
    print("‚Ä¢ Gated Attention: Perspective-specific attention weighting")
    print("‚Ä¢ Multi-Head Architecture: Captures different aspects of medical information")
    print("‚Ä¢ Transformer-based: State-of-the-art sequence-to-sequence architecture")
    print("‚Ä¢ Lightweight Design: Optimized for real-time inference")
    
    return model, tokenizer

def save_demo_model(model, tokenizer):
    """
    Save a demo model for the web application
    """
    print("\nSaving demo model for web application...")
    
    # Save model state
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab_size': tokenizer.vocab_size_actual,
        'model_config': {
            'd_model': 128,
            'num_heads': 4,
            'num_encoder_layers': 2,
            'num_decoder_layers': 2,
            'd_ff': 256,
            'dropout': 0.1
        }
    }, "./model/demo_perspective_aware_model.pth")
    
    print("‚úì Demo model saved to ./model/demo_perspective_aware_model.pth")
    print("‚úì Ready for integration with FastAPI backend")

if __name__ == "__main__":
    model, tokenizer = demonstrate_perspective_awareness()
    save_demo_model(model, tokenizer)

